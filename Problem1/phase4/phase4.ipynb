{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport html\nimport tarfile\nimport torch\nimport torch.nn as nn\nimport math\nimport matplotlib.pyplot as plt\nimport time\n\nPROJECT_PATH = '/kaggle/working/phase_safe'\nCHECKPOINT_PATH = os.path.join(PROJECT_PATH, 'checkpoints')\nDATA_SAVE_PATH = os.path.join(PROJECT_PATH, 'data')\n\nif not os.path.exists(CHECKPOINT_PATH): os.makedirs(CHECKPOINT_PATH)\nif not os.path.exists(DATA_SAVE_PATH): os.makedirs(DATA_SAVE_PATH)\n\nprint(f\"Safe Mode Setup Complete. Dir: {PROJECT_PATH}\")\n\n!pip install -q tokenizers sacrebleu torchtext","metadata":{"colab":{"background_save":true},"id":"zJLMkooRIRKk","outputId":"508cae99-2563-4b04-ac92-9c3438cd6ece","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T01:44:14.434364Z","iopub.execute_input":"2025-12-22T01:44:14.435119Z","iopub.status.idle":"2025-12-22T01:44:21.431637Z","shell.execute_reply.started":"2025-12-22T01:44:14.435085Z","shell.execute_reply":"2025-12-22T01:44:21.430631Z"}},"outputs":[{"name":"stdout","text":"Safe Mode Setup Complete. Dir: /kaggle/working/phase_safe\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from tokenizers import ByteLevelBPETokenizer\nfrom tokenizers.processors import TemplateProcessing\nfrom torch.utils.data import Dataset, DataLoader\n\ndef prepare_data():\n    data_dir = os.path.join(PROJECT_PATH, 'raw_data')\n    if not os.path.exists(data_dir): os.makedirs(data_dir)\n    tgz_path = os.path.join(data_dir, \"train-en-vi.tgz\")\n    \n    if not os.path.exists(os.path.join(data_dir, \"train.vi\")):\n        print(\"Downloading data...\")\n        os.system(f\"wget -q https://github.com/stefan-it/nmt-en-vi/raw/master/data/train-en-vi.tgz -O {tgz_path}\")\n        with tarfile.open(tgz_path, \"r:gz\") as tar:\n            tar.extractall(path=data_dir)\n            \n    clean_paths = {}\n    for lang in ['vi', 'en']:\n        raw_path = os.path.join(data_dir, f\"train.{lang}\")\n        clean_path = os.path.join(DATA_SAVE_PATH, f\"clean_train.{lang}\")\n        if not os.path.exists(clean_path):\n            with open(raw_path, 'r', encoding='utf-8') as f_in, open(clean_path, 'w', encoding='utf-8') as f_out:\n                for line in f_in:\n                    clean_line = html.unescape(line).replace('\\xa0', ' ').strip()\n                    if clean_line:\n                        f_out.write(clean_line + \"\\n\")\n        clean_paths[lang] = clean_path\n    return clean_paths['vi'], clean_paths['en']\n\nclean_vi, clean_en = prepare_data()\n\ndef get_tokenizer(files, name):\n    vocab_file = os.path.join(DATA_SAVE_PATH, f\"{name}-vocab.json\")\n    merges_file = os.path.join(DATA_SAVE_PATH, f\"{name}-merges.txt\")\n    if os.path.exists(vocab_file):\n        tok = ByteLevelBPETokenizer(vocab_file, merges_file)\n    else:\n        print(f\"Training tokenizer: {name}...\")\n        tok = ByteLevelBPETokenizer()\n        # Giảm vocab size xuống 8000 để mô hình tập trung hơn (Safe choice)\n        tok.train(files=files, vocab_size=8000, min_frequency=2, special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"])\n        tok.save_model(DATA_SAVE_PATH, name)\n    return tok\n\nsrc_tokenizer = get_tokenizer([clean_vi], \"src_bpe\")\ntrg_tokenizer = get_tokenizer([clean_en], \"trg_bpe\")\n\nfor tok in [src_tokenizer, trg_tokenizer]:\n    tok.post_processor = TemplateProcessing(\n        single=\"<sos> $A <eos>\",\n        special_tokens=[(\"<sos>\", 1), (\"<eos>\", 2)],\n    )\n    tok.enable_padding(pad_id=0, pad_token=\"<pad>\")\n\nclass BPEDataset(Dataset):\n    def __init__(self, src_file, trg_file, src_tok, trg_tok, max_len=128): # Giảm max_len chút cho nhẹ\n        self.src_data = []\n        self.trg_data = []\n        self.src_tok = src_tok\n        self.trg_tok = trg_tok\n        self.max_len = max_len\n        with open(src_file, 'r', encoding='utf-8') as f: src_lines = f.readlines()\n        with open(trg_file, 'r', encoding='utf-8') as f: trg_lines = f.readlines()\n        for s, t in zip(src_lines, trg_lines):\n            self.src_data.append(s.strip())\n            self.trg_data.append(t.strip())\n\n    def __len__(self): return len(self.src_data)\n\n    def __getitem__(self, index):\n        src_ids = self.src_tok.encode(self.src_data[index]).ids\n        trg_ids = self.trg_tok.encode(self.trg_data[index]).ids\n        if len(src_ids) > self.max_len: src_ids = src_ids[:self.max_len]\n        if len(trg_ids) > self.max_len: trg_ids = trg_ids[:self.max_len]\n        return torch.tensor(src_ids), torch.tensor(trg_ids)\n\ndef collate_fn(batch):\n    src_batch, trg_batch = [], []\n    for s, t in batch:\n        src_batch.append(s)\n        trg_batch.append(t)\n    src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=0, batch_first=True)\n    trg_batch = torch.nn.utils.rnn.pad_sequence(trg_batch, padding_value=0, batch_first=True)\n    return src_batch, trg_batch\n\ntrain_dataset = BPEDataset(clean_vi, clean_en, src_tokenizer, trg_tokenizer)\ntrain_iterator = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, num_workers=2)","metadata":{"colab":{"background_save":true},"id":"yatK-0YkIUit","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T01:44:21.433639Z","iopub.execute_input":"2025-12-22T01:44:21.434173Z","iopub.status.idle":"2025-12-22T01:44:30.038699Z","shell.execute_reply.started":"2025-12-22T01:44:21.434143Z","shell.execute_reply":"2025-12-22T01:44:30.037825Z"}},"outputs":[{"name":"stdout","text":"Downloading data...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/3175411450.py:14: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n  tar.extractall(path=data_dir)\n","output_type":"stream"},{"name":"stdout","text":"Training tokenizer: src_bpe...\n\n\n\nTraining tokenizer: trg_bpe...\n\n\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n    def forward(self, x): return x + self.pe[:, :x.size(1)]\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_head):\n        super().__init__()\n        self.d_head = d_model // n_head\n        self.n_head = n_head\n        self.d_model = d_model\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.fc_out = nn.Linear(d_model, d_model)\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.shape[0]\n        Q = self.w_q(query).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n        K = self.w_k(key).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n        V = self.w_v(value).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.d_head)\n        if mask is not None: energy = energy.masked_fill(mask == 0, -1e9)\n        attention = torch.softmax(energy, dim=-1)\n        x = torch.matmul(attention, V)\n        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n        return self.fc_out(x)\n\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n    def forward(self, x): return self.fc2(self.relu(self.fc1(x)))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, n_head, d_ff, dropout):\n        super().__init__()\n        self.attn = MultiHeadAttention(d_model, n_head)\n        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, src, mask):\n        _src = self.attn(src, src, src, mask)\n        src = self.norm1(src + self.dropout(_src))\n        _src = self.ffn(src)\n        src = self.norm2(src + self.dropout(_src))\n        return src\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, n_head, d_ff, dropout):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, n_head)\n        self.cross_attn = MultiHeadAttention(d_model, n_head)\n        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, trg, enc_src, trg_mask, src_mask):\n        _trg = self.self_attn(trg, trg, trg, trg_mask)\n        trg = self.norm1(trg + self.dropout(_trg))\n        _trg = self.cross_attn(trg, enc_src, enc_src, src_mask)\n        trg = self.norm2(trg + self.dropout(_trg))\n        _trg = self.ffn(trg)\n        trg = self.norm3(trg + self.dropout(_trg))\n        return trg\n\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab, trg_vocab, d_model, n_head, n_layer, d_ff, dropout, max_len, pad_idx):\n        super().__init__()\n        self.src_pad_idx = pad_idx\n        self.trg_pad_idx = pad_idx\n        self.encoder_emb = nn.Embedding(src_vocab, d_model)\n        self.decoder_emb = nn.Embedding(trg_vocab, d_model)\n        self.pos_enc = PositionalEncoding(d_model, max_len)\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layer)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layer)])\n        self.fc_out = nn.Linear(d_model, trg_vocab)\n        self.dropout = nn.Dropout(dropout)\n\n    def make_src_mask(self, src):\n        return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n\n    def make_trg_mask(self, trg):\n        pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n        sub_mask = torch.tril(torch.ones((trg.shape[1], trg.shape[1]), device=trg.device)).bool()\n        return pad_mask & sub_mask\n\n    def encode(self, src, src_mask):\n        src = self.dropout(self.pos_enc(self.encoder_emb(src)))\n        for layer in self.encoder_layers: src = layer(src, src_mask)\n        return src\n\n    def decode(self, trg, enc_src, trg_mask, src_mask):\n        trg = self.dropout(self.pos_enc(self.decoder_emb(trg)))\n        for layer in self.decoder_layers: trg = layer(trg, enc_src, trg_mask, src_mask)\n        return self.fc_out(trg)\n\n    def forward(self, src, trg):\n        src_mask = self.make_src_mask(src)\n        trg_mask = self.make_trg_mask(trg)\n        enc_src = self.encode(src, src_mask)\n        output = self.decode(trg, enc_src, trg_mask, src_mask)\n        return output","metadata":{"colab":{"background_save":true},"id":"M9XLwdI_IWdo","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T01:44:30.039782Z","iopub.execute_input":"2025-12-22T01:44:30.040060Z","iopub.status.idle":"2025-12-22T01:44:30.071943Z","shell.execute_reply.started":"2025-12-22T01:44:30.040027Z","shell.execute_reply":"2025-12-22T01:44:30.071110Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, padding_idx, smoothing=0.1, dim=-1):\n        super().__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.padding_idx = padding_idx\n        self.dim = dim\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            true_dist[:, self.padding_idx] = 0\n            mask = (target == self.padding_idx)\n            if mask.any(): true_dist.masked_fill_(mask.unsqueeze(1), 0.0)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n\nclass NoamScheduler:\n    def __init__(self, optimizer, d_model, warmup_steps=4000, factor=1.0):\n        self.optimizer = optimizer\n        self.d_model = d_model\n        self.warmup_steps = warmup_steps\n        self.factor = factor\n        self.step_num = 0\n    def step(self):\n        self.step_num += 1\n        lr = self.factor * (self.d_model ** -0.5) * min(self.step_num ** -0.5, self.step_num * (self.warmup_steps ** -1.5))\n        for p in self.optimizer.param_groups: p['lr'] = lr","metadata":{"colab":{"background_save":true},"id":"e8RXeStjIYCt","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T01:44:30.073610Z","iopub.execute_input":"2025-12-22T01:44:30.073887Z","iopub.status.idle":"2025-12-22T01:44:30.092987Z","shell.execute_reply.started":"2025-12-22T01:44:30.073863Z","shell.execute_reply":"2025-12-22T01:44:30.092305Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nPAD_IDX = 0\n\nD_MODEL = 256\nN_HEAD = 8\nN_LAYER = 3          \nD_FF = 512          \nDROPOUT = 0.3        \nMAX_LEN = 128\nWARMUP_STEPS = 4000\nN_EPOCHS = 15       \n\nINPUT_DIM = src_tokenizer.get_vocab_size()\nOUTPUT_DIM = trg_tokenizer.get_vocab_size()\n\nmodel = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, N_HEAD, N_LAYER, D_FF, DROPOUT, MAX_LEN, PAD_IDX).to(DEVICE)\n\ndef init_weights(m):\n    if hasattr(m, 'weight') and m.weight.dim() > 1:\n        nn.init.xavier_uniform_(m.weight.data)\nmodel.apply(init_weights)\n\nprint(f\"Params (Safe Mode): {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\nscheduler = NoamScheduler(optimizer, D_MODEL, WARMUP_STEPS)\ncriterion = LabelSmoothingLoss(OUTPUT_DIM, PAD_IDX, smoothing=0.1)\n\ndef train_epoch(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    for i, (src, trg) in enumerate(iterator):\n        src, trg = src.to(DEVICE), trg.to(DEVICE)\n        trg_in = trg[:, :-1]\n        trg_out = trg[:, 1:]\n        optimizer.zero_grad()\n        output = model(src, trg_in)\n        output = output.contiguous().view(-1, output.shape[-1])\n        trg_out = trg_out.contiguous().view(-1)\n        loss = criterion(output, trg_out)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        scheduler.step()\n        epoch_loss += loss.item()\n    return epoch_loss / len(iterator)\n\nbest_loss = float('inf')\nlosses = []\n\nprint(\"Training Safe Mode...\")\nfor epoch in range(N_EPOCHS):\n    start = time.time()\n    loss = train_epoch(model, train_iterator, optimizer, criterion, 1.0)\n    end = time.time()\n    losses.append(loss)\n    print(f\"Epoch {epoch+1:02} | Time: {(end-start)/60:.1f}m | Loss: {loss:.4f}\")\n    \n    if loss < best_loss:\n        best_loss = loss\n        torch.save(model.state_dict(), os.path.join(CHECKPOINT_PATH, 'transformer_safe_best.pt'))\n\nplt.plot(losses)\nplt.savefig(os.path.join(PROJECT_PATH, 'loss_safe.png'))\nplt.show()","metadata":{"colab":{"background_save":true},"id":"p4PoGZMhIZlz","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T01:44:30.093845Z","iopub.execute_input":"2025-12-22T01:44:30.094125Z","iopub.status.idle":"2025-12-22T02:43:54.462099Z","shell.execute_reply.started":"2025-12-22T01:44:30.094102Z","shell.execute_reply":"2025-12-22T02:43:54.461416Z"}},"outputs":[{"name":"stdout","text":"Params (Safe Mode): 10,105,664\nTraining Safe Mode...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Time: 3.9m | Loss: 1.9734\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Time: 4.0m | Loss: 1.7140\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Time: 4.0m | Loss: 1.6391\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Time: 4.0m | Loss: 1.5902\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Time: 4.0m | Loss: 1.5695\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Time: 4.0m | Loss: 1.5485\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Time: 4.0m | Loss: 1.5273\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Time: 4.0m | Loss: 1.5229\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Time: 3.9m | Loss: 1.5131\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Time: 4.0m | Loss: 1.4996\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Time: 4.0m | Loss: 1.4920\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Time: 4.0m | Loss: 1.4907\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Time: 4.0m | Loss: 1.4820\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Time: 4.0m | Loss: 1.4810\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Time: 4.0m | Loss: 1.4666\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Inference+Evaluate","metadata":{}},{"cell_type":"code","source":"import sacrebleu\nfrom tqdm import tqdm\nimport math\nimport urllib.request \nimport tarfile\n\nprint(\"Loading Best Model...\")\nbest_model_path = os.path.join(CHECKPOINT_PATH, 'transformer_safe_best.pt')\nif os.path.exists(best_model_path):\n    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n    print(\"Model loaded!\")\nelse:\n    print(\"WARNING: Không tìm thấy model checkpoint, đang dùng weight hiện tại.\")\n\nmodel.eval()\n\ndef beam_search_decode(sentence, model, device, beam_size=3, max_len=100):\n    model.eval()\n    src_ids = src_tokenizer.encode(sentence).ids\n    src_tensor = torch.LongTensor(src_ids).unsqueeze(0).to(device)\n    src_mask = model.make_src_mask(src_tensor)\n    \n    with torch.no_grad():\n        enc_src = model.encode(src_tensor, src_mask)\n        beam = [([1], 0.0)] # SOS_IDX = 1\n        \n        for i in range(max_len):\n            candidates = []\n            all_ended = True\n            for seq, score in beam:\n                if seq[-1] == 2: # EOS_IDX = 2\n                    candidates.append((seq, score))\n                    continue\n                all_ended = False\n                trg_tensor = torch.LongTensor(seq).unsqueeze(0).to(device)\n                trg_mask = model.make_trg_mask(trg_tensor)\n                output = model.decode(trg_tensor, enc_src, trg_mask, src_mask)\n                prob = output[:, -1, :]\n                log_prob = torch.log_softmax(prob, dim=1).squeeze(0)\n                topk_prob, topk_idx = torch.topk(log_prob, beam_size)\n                for j in range(beam_size):\n                    token = topk_idx[j].item()\n                    p = topk_prob[j].item()\n                    candidates.append((seq + [token], score + p))\n            \n            if all_ended: break\n            beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n            \n    best_seq = beam[0][0]\n    return trg_tokenizer.decode(best_seq, skip_special_tokens=True)\n\ntest_dir = os.path.join(PROJECT_PATH, 'test_data')\nif not os.path.exists(test_dir): os.makedirs(test_dir)\ntgz_path = os.path.join(test_dir, 'test.tgz')\n\nif not os.path.exists(tgz_path) or os.path.getsize(tgz_path) < 1000:\n    print(\"Downloading Test Data...\")\n    url = \"https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz\"\n    try:\n        urllib.request.urlretrieve(url, tgz_path)\n        print(\"Download thành công!\")\n    except Exception as e:\n        print(f\"LỖI TẢI FILE: {e}\")\n        print(\"Vui lòng kiểm tra lại kết nối Internet của Kaggle (Settings -> Internet -> On)\")\n        raise e\n\nif not os.path.exists(os.path.join(test_dir, 'tst2013.vi')):\n    print(\"Extracting...\")\n    with tarfile.open(tgz_path, \"r:gz\") as tar: \n        tar.extractall(path=test_dir)\n\nwith open(os.path.join(test_dir, 'tst2013.vi'), 'r', encoding='utf-8') as f:\n    src_sents = [html.unescape(line).replace('\\xa0', ' ').strip() for line in f]\nwith open(os.path.join(test_dir, 'tst2013.en'), 'r', encoding='utf-8') as f:\n    ref_sents = [html.unescape(line).replace('\\xa0', ' ').strip() for line in f]\n\nprint(f\"Evaluating Safe Mode with BEAM SEARCH (k=3)...\")\nhypotheses = []\n\nfor sent in tqdm(src_sents):\n    pred = beam_search_decode(sent, model, DEVICE, beam_size=3)\n    hypotheses.append(pred)\n\nbleu = sacrebleu.corpus_bleu(hypotheses, [ref_sents], tokenize='13a')\n\nprint(f\"\\n{'='*40}\")\nprint(f\"SAFE MODE + BEAM SEARCH RESULT\")\nprint(f\"BLEU SCORE: {bleu.score:.2f}\")\nprint(f\"{'='*40}\")\n\nprint(\"\\n--- Examples ---\")\nfor i in range(3):\n    print(f\"SRC : {src_sents[i]}\")\n    print(f\"REF : {ref_sents[i]}\")\n    print(f\"PRED: {hypotheses[i]}\")\n    print(\"-\" * 20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T02:47:13.174855Z","iopub.execute_input":"2025-12-22T02:47:13.175703Z","iopub.status.idle":"2025-12-22T02:51:55.206427Z","shell.execute_reply.started":"2025-12-22T02:47:13.175673Z","shell.execute_reply":"2025-12-22T02:51:55.205824Z"}},"outputs":[{"name":"stdout","text":"Loading Best Model...\nModel loaded!\nDownloading Test Data...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2169181502.py:70: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n  tar.extractall(path=test_dir)\n","output_type":"stream"},{"name":"stdout","text":"Download thành công!\nExtracting...\nEvaluating Safe Mode with BEAM SEARCH (k=3)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1268/1268 [04:41<00:00,  4.50it/s]\nThat's 100 lines that end in a tokenized period ('.')\nIt looks like you forgot to detokenize your test data, which may hurt your score.\nIf you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","output_type":"stream"},{"name":"stdout","text":"\n========================================\nSAFE MODE + BEAM SEARCH RESULT\nBLEU SCORE: 0.54\n========================================\n\n--- Examples ---\nSRC : Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài \" Chúng ta chẳng có gì phải ghen tị . \"\nREF : When I was little , I thought my country was the best on the planet , and I grew up singing a song called \" Nothing To Envy . \"\nPRED: So I 'm going to show you a little bit about what I want to do .\n--------------------\nSRC : Tôi đã rất tự hào về đất nước tôi .\nREF : And I was very proud .\nPRED: So I 'm going to show you a little bit about what I want to do .\n--------------------\nSRC : Ở trường , chúng tôi dành rất nhiều thời gian để học về cuộc đời của chủ tịch Kim II- Sung , nhưng lại không học nhiều về thế giới bên ngoài , ngoại trừ việc Hoa Kỳ , Hàn Quốc và Nhật Bản là kẻ thù của chúng tôi .\nREF : In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\nPRED: So I 'm going to show you a little bit about what I want to do .\n--------------------\n","output_type":"stream"}],"execution_count":10}]}