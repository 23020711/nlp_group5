{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL_xrlUYBXnm",
        "outputId": "a3d40ef2-c716-48ca-dfb1-4409bd1b319d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_PATH = '/content/drive/MyDrive/NLP_Assignment_2025(phase3)'\n",
        "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
        "CHECKPOINT_PATH = os.path.join(PROJECT_PATH, 'checkpoints')\n",
        "MODEL_FILE = os.path.join(CHECKPOINT_PATH, 'transformer_phase3_best.pt')\n",
        "\n",
        "!pip install -q tokenizers sacrebleu tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "print(\"Loading BPE Tokenizers...\")\n",
        "try:\n",
        "    src_tokenizer = ByteLevelBPETokenizer(\n",
        "        os.path.join(DATA_PATH, \"src_bpe-vocab.json\"),\n",
        "        os.path.join(DATA_PATH, \"src_bpe-merges.txt\")\n",
        "    )\n",
        "    trg_tokenizer = ByteLevelBPETokenizer(\n",
        "        os.path.join(DATA_PATH, \"trg_bpe-vocab.json\"),\n",
        "        os.path.join(DATA_PATH, \"trg_bpe-merges.txt\")\n",
        "    )\n",
        "    src_tokenizer.post_processor = TemplateProcessing(\n",
        "        single=\"<sos> $A <eos>\",\n",
        "        special_tokens=[(\"<sos>\", 1), (\"<eos>\", 2)],\n",
        "    )\n",
        "    print(\"Tokenizers loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi load tokenizer: {e}\")\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_head = d_model // n_head\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.w_q(query).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        K = self.w_k(key).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        V = self.w_v(value).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.d_head)\n",
        "        if mask is not None: energy = energy.masked_fill(mask == 0, -1e9)\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        x = torch.matmul(attention, V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.fc_out(x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x): return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src, src_mask):\n",
        "        _src = self.self_attn(src, src, src, src_mask)\n",
        "        src = self.norm1(src + self.dropout(_src))\n",
        "        _src = self.ffn(src)\n",
        "        src = self.norm2(src + self.dropout(_src))\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        _trg = self.self_attn(trg, trg, trg, trg_mask)\n",
        "        trg = self.norm1(trg + self.dropout(_trg))\n",
        "        _trg = self.cross_attn(trg, enc_src, enc_src, src_mask)\n",
        "        trg = self.norm2(trg + self.dropout(_trg))\n",
        "        _trg = self.ffn(trg)\n",
        "        trg = self.norm3(trg + self.dropout(_trg))\n",
        "        return trg\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_layer, n_head, d_ff, dropout, max_len):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layer)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.dropout(self.pos_encoding(self.embedding(src)))\n",
        "        for layer in self.layers: src = layer(src, src_mask)\n",
        "        return src\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, d_model, n_layer, n_head, d_ff, dropout, max_len):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layer)])\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        trg = self.dropout(self.pos_encoding(self.embedding(trg)))\n",
        "        for layer in self.layers: trg = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        return self.fc_out(trg)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model=512, n_head=8, n_layer=6, d_ff=2048, dropout=0.1, max_len=100, src_pad_idx=0, trg_pad_idx=0):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, n_layer, n_head, d_ff, dropout, max_len)\n",
        "        self.decoder = Decoder(trg_vocab_size, d_model, n_layer, n_head, d_ff, dropout, max_len)\n",
        "    def make_src_mask(self, src): return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=trg.device)).bool()\n",
        "        return trg_pad_mask & trg_sub_mask\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        return output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wd3eIVsBfbd",
        "outputId": "65b95984-3216-4751-9452-cc90633d99ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BPE Tokenizers...\n",
            "Tokenizers loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "PAD_IDX = 0\n",
        "SOS_IDX = 1\n",
        "EOS_IDX = 2\n",
        "\n",
        "print(\"Loading Model Phase 3...\")\n",
        "input_dim = src_tokenizer.get_vocab_size()\n",
        "output_dim = trg_tokenizer.get_vocab_size()\n",
        "\n",
        "model = Transformer(input_dim, output_dim, d_model=512, n_head=8, n_layer=6, d_ff=2048, dropout=0.1, max_len=150, src_pad_idx=PAD_IDX, trg_pad_idx=PAD_IDX)\n",
        "\n",
        "if os.path.exists(MODEL_FILE):\n",
        "    model.load_state_dict(torch.load(MODEL_FILE, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Error: Model file not found at {MODEL_FILE}\")\n",
        "\n",
        "def beam_search(sentence, model, device, beam_size=3, max_len=100):\n",
        "    model.eval()\n",
        "\n",
        "    src_encoded = src_tokenizer.encode(sentence)\n",
        "    src_tensor = torch.LongTensor(src_encoded.ids).unsqueeze(0).to(device)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "        beam = [([SOS_IDX], 0.0)]\n",
        "\n",
        "        for i in range(max_len):\n",
        "            candidates = []\n",
        "            all_ended = True\n",
        "\n",
        "            for seq, score in beam:\n",
        "                if seq[-1] == EOS_IDX:\n",
        "                    candidates.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                all_ended = False\n",
        "                trg_tensor = torch.LongTensor(seq).unsqueeze(0).to(device)\n",
        "                trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "                output = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "                prob = output[:, -1, :]\n",
        "                log_prob = torch.log_softmax(prob, dim=1).squeeze(0)\n",
        "\n",
        "                topk_prob, topk_idx = torch.topk(log_prob, beam_size)\n",
        "\n",
        "                for j in range(beam_size):\n",
        "                    token = topk_idx[j].item()\n",
        "                    token_prob = topk_prob[j].item()\n",
        "                    candidates.append((seq + [token], score + token_prob))\n",
        "\n",
        "            if all_ended: break\n",
        "\n",
        "            beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "    best_seq = beam[0][0]\n",
        "    decoded_text = trg_tokenizer.decode(best_seq, skip_special_tokens=True)\n",
        "    return decoded_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlzPo2OrBkp6",
        "outputId": "aa0fdf82-d5e4-4e41-eec9-a44d4f47a66e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model Phase 3...\n",
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import html\n",
        "\n",
        "TEST_DIR = os.path.join(PROJECT_PATH, 'data', 'test_2013')\n",
        "if not os.path.exists(TEST_DIR): os.makedirs(TEST_DIR)\n",
        "tgz_path = os.path.join(TEST_DIR, \"test.tgz\")\n",
        "\n",
        "if not os.path.exists(os.path.join(TEST_DIR, 'tst2013.vi')):\n",
        "    print(\"Downloading test data...\")\n",
        "    urllib.request.urlretrieve(\"https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz\", tgz_path)\n",
        "    with tarfile.open(tgz_path, \"r:gz\") as tar: tar.extractall(path=TEST_DIR)\n",
        "\n",
        "def clean_text(text): return html.unescape(text).replace('\\xa0', ' ').strip()\n",
        "\n",
        "with open(os.path.join(TEST_DIR, 'tst2013.vi'), 'r', encoding='utf-8') as f:\n",
        "    src_sents = [clean_text(line) for line in f.readlines()]\n",
        "with open(os.path.join(TEST_DIR, 'tst2013.en'), 'r', encoding='utf-8') as f:\n",
        "    ref_sents = [clean_text(line) for line in f.readlines()]\n",
        "\n",
        "print(f\"Evaluating Phase 3 (BPE + Beam Search) on {len(src_sents)} sentences...\")\n",
        "\n",
        "hypotheses = []\n",
        "for sent in tqdm(src_sents):\n",
        "    pred = beam_search(sent, model, DEVICE, beam_size=3)\n",
        "    hypotheses.append(pred)\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(hypotheses, [ref_sents], tokenize='13a')\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(f\"FINAL BLEU SCORE (PHASE 3): {bleu.score:.2f}\")\n",
        "print(f\"{'='*40}\")\n",
        "\n",
        "# In ví dụ\n",
        "for i in range(3):\n",
        "    print(f\"SRC : {src_sents[i]}\")\n",
        "    print(f\"REF : {ref_sents[i]}\")\n",
        "    print(f\"PRED: {hypotheses[i]}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRfCDkgoBsCk",
        "outputId": "01b7e06e-a872-4d9a-cb87-04fc6ff22f1a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Phase 3 (BPE + Beam Search) on 1268 sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1268/1268 [08:16<00:00,  2.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "FINAL BLEU SCORE (PHASE 3): 0.26\n",
            "========================================\n",
            "SRC : Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài \" Chúng ta chẳng có gì phải ghen tị . \"\n",
            "REF : When I was little , I thought my country was the best on the planet , and I grew up singing a song called \" Nothing To Envy . \"\n",
            "PRED: <sos>And I 'm going to show you a little bit of this .<eos>\n",
            "--------------------\n",
            "SRC : Tôi đã rất tự hào về đất nước tôi .\n",
            "REF : And I was very proud .\n",
            "PRED: <sos>And I 'm going to show you a little bit of this .<eos>\n",
            "--------------------\n",
            "SRC : Ở trường , chúng tôi dành rất nhiều thời gian để học về cuộc đời của chủ tịch Kim II- Sung , nhưng lại không học nhiều về thế giới bên ngoài , ngoại trừ việc Hoa Kỳ , Hàn Quốc và Nhật Bản là kẻ thù của chúng tôi .\n",
            "REF : In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\n",
            "PRED: <sos>And I 'm going to show you a little bit of this .<eos>\n",
            "--------------------\n"
          ]
        }
      ]
    }
  ]
}