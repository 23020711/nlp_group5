{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "import tarfile\n",
        "import html\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import drive\n",
        "\n",
        "from pyvi import ViTokenizer\n",
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_PATH = '/content/drive/MyDrive/NLP_Phase6_Final'\n",
        "CHECKPOINT_PATH = os.path.join(PROJECT_PATH, 'checkpoints')\n",
        "DATA_SAVE_PATH = os.path.join(PROJECT_PATH, 'data')\n",
        "\n",
        "if not os.path.exists(CHECKPOINT_PATH): os.makedirs(CHECKPOINT_PATH)\n",
        "if not os.path.exists(DATA_SAVE_PATH): os.makedirs(DATA_SAVE_PATH)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Running on: {DEVICE}\")\n",
        "\n",
        "def download_and_clean():\n",
        "    url = \"https://github.com/stefan-it/nmt-en-vi/raw/master/data/train-en-vi.tgz\"\n",
        "    tgz_path = os.path.join(DATA_SAVE_PATH, \"train-en-vi.tgz\")\n",
        "\n",
        "    if not os.path.exists(os.path.join(DATA_SAVE_PATH, \"train.vi\")):\n",
        "        print(\"Downloading data...\")\n",
        "        os.system(f\"wget -q {url} -O {tgz_path}\")\n",
        "        with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
        "            tar.extractall(path=DATA_SAVE_PATH)\n",
        "\n",
        "    def clean_data(path):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            return [html.unescape(line).replace('\\xa0', ' ').strip() for line in f]\n",
        "\n",
        "    src_lines = clean_data(os.path.join(DATA_SAVE_PATH, 'train.vi'))\n",
        "    trg_lines = clean_data(os.path.join(DATA_SAVE_PATH, 'train.en'))\n",
        "    return src_lines, trg_lines\n",
        "\n",
        "src_raw, trg_raw = download_and_clean()\n",
        "print(f\"Loaded {len(src_raw)} sentences.\")\n",
        "\n",
        "def tokenize_vi(text):\n",
        "    return ViTokenizer.tokenize(text).lower().split()\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "# -----------------------------\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=2, lang='vi'):\n",
        "        self.itos = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
        "        self.stoi = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.lang = lang\n",
        "\n",
        "    def __len__(self): return len(self.itos)\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        for sentence in sentence_list:\n",
        "            if self.lang == 'vi':\n",
        "                tokens = tokenize_vi(sentence)\n",
        "            else:\n",
        "                tokens = tokenize_en(sentence)\n",
        "            frequencies.update(tokens)\n",
        "\n",
        "        idx = 4\n",
        "        for word, freq in frequencies.items():\n",
        "            if freq >= self.freq_threshold:\n",
        "                self.stoi[word] = idx\n",
        "                self.itos[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        if self.lang == 'vi':\n",
        "            tokens = tokenize_vi(text)\n",
        "        else:\n",
        "            tokens = tokenize_en(text)\n",
        "        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokens]\n",
        "\n",
        "src_vocab = Vocabulary(freq_threshold=2, lang='vi')\n",
        "trg_vocab = Vocabulary(freq_threshold=2, lang='en')\n",
        "src_vocab.build_vocabulary(src_raw)\n",
        "trg_vocab.build_vocabulary(trg_raw)\n",
        "print(f\"Vocab Size: Vi={len(src_vocab)}, En={len(trg_vocab)}\")\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_lines, trg_lines, src_vocab, trg_vocab, max_len=100):\n",
        "        self.src_lines = src_lines\n",
        "        self.trg_lines = trg_lines\n",
        "        self.src_vocab = src_vocab\n",
        "        self.trg_vocab = trg_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.src_lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_idx = [1] + self.src_vocab.numericalize(self.src_lines[idx]) + [2]\n",
        "        trg_idx = [1] + self.trg_vocab.numericalize(self.trg_lines[idx]) + [2]\n",
        "        return torch.tensor(src_idx[:self.max_len]), torch.tensor(trg_idx[:self.max_len])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = [], []\n",
        "    for src, trg in batch:\n",
        "        src_batch.append(src)\n",
        "        trg_batch.append(trg)\n",
        "    src_pad = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=0, batch_first=True)\n",
        "    trg_pad = torch.nn.utils.rnn.pad_sequence(trg_batch, padding_value=0, batch_first=True)\n",
        "    return src_pad, trg_pad\n",
        "\n",
        "train_loader = DataLoader(TranslationDataset(src_raw, trg_raw, src_vocab, trg_vocab),\n",
        "                          batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX4jFNd_R8LK",
        "outputId": "c2a51000-4a5c-4edf-c1bc-392dc6686b8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvi in /usr/local/lib/python3.12/dist-packages (0.1.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from pyvi) (1.6.1)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.12/dist-packages (from pyvi) (0.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pyvi) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pyvi) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pyvi) (3.6.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.7 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite->pyvi) (0.9.11)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Running on: cuda\n",
            "Loaded 133317 sentences.\n",
            "Vocab Size: Vi=20724, En=28162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_head = d_model // n_head\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.w_q(query).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        K = self.w_k(key).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        V = self.w_v(value).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.d_head)\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        x = torch.matmul(attention, V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.fc_out(x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        _src = self.self_attn(src, src, src, src_mask)\n",
        "        src = self.norm1(src + self.dropout(_src))\n",
        "        _src = self.ffn(src)\n",
        "        src = self.norm2(src + self.dropout(_src))\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        _trg = self.self_attn(trg, trg, trg, trg_mask)\n",
        "        trg = self.norm1(trg + self.dropout(_trg))\n",
        "        _trg = self.cross_attn(trg, enc_src, enc_src, src_mask)\n",
        "        trg = self.norm2(trg + self.dropout(_trg))\n",
        "        _trg = self.ffn(trg)\n",
        "        trg = self.norm3(trg + self.dropout(_trg))\n",
        "        return trg\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model=256, n_head=8, n_layer=3, d_ff=512, dropout=0.1, max_len=150):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layer)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layer)])\n",
        "\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.src_pad_idx = 0\n",
        "        self.trg_pad_idx = 0\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=trg.device)).bool()\n",
        "        return trg_pad_mask & trg_sub_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        src = self.dropout(self.pos_encoding(self.src_embedding(src)))\n",
        "        for layer in self.encoder:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        trg = self.dropout(self.pos_encoding(self.trg_embedding(trg)))\n",
        "        for layer in self.decoder:\n",
        "            trg = layer(trg, src, trg_mask, src_mask)\n",
        "\n",
        "        return self.fc_out(trg)"
      ],
      "metadata": {
        "id": "op8GiUINR9fF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# 1. Khởi tạo lại Model và các thành phần y như cũ\n",
        "D_MODEL = 256\n",
        "model = Transformer(len(src_vocab), len(trg_vocab), d_model=D_MODEL, n_head=8, n_layer=3).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = NoamScheduler(optimizer, d_model=D_MODEL, warmup_steps=4000)\n",
        "criterion = LabelSmoothingLoss(len(trg_vocab), padding_idx=0, smoothing=0.1)\n",
        "\n",
        "# 2. Đường dẫn file save\n",
        "save_path = os.path.join(CHECKPOINT_PATH, 'phase6_safe_best.pt')\n",
        "\n",
        "# 3. Load Checkpoint\n",
        "if os.path.exists(save_path):\n",
        "    print(f\"--> Đang load checkpoint từ: {save_path}\")\n",
        "    state_dict = torch.load(save_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(\"--> Load model thành công!\")\n",
        "else:\n",
        "    print(\"--> KHÔNG TÌM THẤY CHECKPOINT! Kiểm tra lại đường dẫn.\")\n",
        "\n",
        "START_EPOCH = 20\n",
        "TOTAL_EPOCHS = 25\n",
        "\n",
        "\n",
        "steps_finished = START_EPOCH * len(train_loader)\n",
        "scheduler.step_num = steps_finished\n",
        "print(f\"--> Đã khôi phục Scheduler về step thứ: {steps_finished}\")\n",
        "\n",
        "print(f\"Tiếp tục Training Phase 6 từ Epoch {START_EPOCH + 1}...\")\n",
        "\n",
        "for epoch in range(START_EPOCH, TOTAL_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for src, trg in train_loader:\n",
        "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "\n",
        "        trg_input = trg[:, :-1]\n",
        "        trg_output = trg[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg_input)\n",
        "\n",
        "        output = output.contiguous().view(-1, output.shape[-1])\n",
        "        trg_output = trg_output.contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, trg_output)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02} | Time: {(time.time()-start_time)/60:.1f}m | Loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    new_save_path = os.path.join(CHECKPOINT_PATH, 'phase6_epoch25.pt') # Tên file mới\n",
        "    torch.save(model.state_dict(), new_save_path)\n",
        "    print(f\"--> Đã lưu checkpoint epoch {epoch+1} vào file mới\")"
      ],
      "metadata": {
        "id": "kfm2ndVMR_tp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "outputId": "8d9199bb-13a9-4454-ba17-c762984bd62d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Đang load checkpoint từ: /content/drive/MyDrive/NLP_Phase6_Final/checkpoints/phase6_safe_best.pt\n",
            "--> Load model thành công!\n",
            "--> Đã khôi phục Scheduler về step thứ: 83340\n",
            "Tiếp tục Training Phase 6 từ Epoch 21...\n",
            "Epoch 21 | Time: 6.2m | Loss: 1.3023\n",
            "--> Đã lưu checkpoint epoch 21 vào file mới\n",
            "Epoch 22 | Time: 6.2m | Loss: 1.2886\n",
            "--> Đã lưu checkpoint epoch 22 vào file mới\n",
            "Epoch 23 | Time: 6.1m | Loss: 1.2886\n",
            "--> Đã lưu checkpoint epoch 23 vào file mới\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2378981344.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtrg_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-871284928.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtrue_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtrue_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrue_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrue_dist\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_decode(model, src_sentence, src_vocab, trg_vocab, device, beam_width=3, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    tokens = [1] + src_vocab.numericalize(src_sentence) + [2]\n",
        "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_emb = model.dropout(model.pos_encoding(model.src_embedding(src_tensor)))\n",
        "        enc_src = src_emb\n",
        "        for layer in model.encoder:\n",
        "            enc_src = layer(enc_src, src_mask)\n",
        "\n",
        "    sequences = [([1], 0.0, False)]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        all_candidates = []\n",
        "\n",
        "        for seq, score, finished in sequences:\n",
        "            if finished:\n",
        "                all_candidates.append((seq, score, True))\n",
        "                continue\n",
        "\n",
        "            trg_tensor = torch.LongTensor(seq).unsqueeze(0).to(device)\n",
        "            trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                trg_emb = model.dropout(model.pos_encoding(model.trg_embedding(trg_tensor)))\n",
        "                out = trg_emb\n",
        "                for layer in model.decoder:\n",
        "                    out = layer(out, enc_src, trg_mask, src_mask)\n",
        "                output = model.fc_out(out)\n",
        "\n",
        "            prob = output[:, -1, :].log_softmax(dim=-1)\n",
        "\n",
        "            topk_probs, topk_idxs = prob.topk(beam_width)\n",
        "\n",
        "            for i in range(beam_width):\n",
        "                token = topk_idxs[0][i].item()\n",
        "                token_prob = topk_probs[0][i].item()\n",
        "\n",
        "                new_seq = seq + [token]\n",
        "                new_score = score + token_prob\n",
        "\n",
        "                if token == 2: # 2 là <eos>\n",
        "                    all_candidates.append((new_seq, new_score, True))\n",
        "                else:\n",
        "                    all_candidates.append((new_seq, new_score, False))\n",
        "\n",
        "        ordered = sorted(all_candidates, key=lambda x: x[1] / (len(x[0])**0.7), reverse=True)\n",
        "        sequences = ordered[:beam_width]\n",
        "\n",
        "        if all([s[2] for s in sequences]):\n",
        "            break\n",
        "\n",
        "    best_seq = sequences[0][0]\n",
        "\n",
        "    words = []\n",
        "    for idx in best_seq:\n",
        "        if idx not in [0, 1, 2, 3]:\n",
        "            words.append(trg_vocab.itos[idx])\n",
        "\n",
        "    decoded_sentence = \" \".join(words)\n",
        "\n",
        "    decoded_sentence = decoded_sentence.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" ?\", \"?\")\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  KẾT QUẢ DỊCH THỬ NGHIỆM (BEAM SEARCH WIDTH=3)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "test_sentences = [\n",
        "    \"Tôi là sinh viên\",\n",
        "    \"Hôm nay trời đẹp\",\n",
        "    \"Bạn có thích học lập trình không?\",\n",
        "    \"Trí tuệ nhân tạo rất thú vị.\",\n",
        "    \"Cảm ơn bạn đã giúp đỡ tôi.\",\n",
        "    \"Tôi muốn đi du lịch.\"\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "for sent in test_sentences:\n",
        "    pred = beam_search_decode(model, sent, src_vocab, trg_vocab, DEVICE, beam_width=3)\n",
        "    print(f\"Input : {sent}\")\n",
        "    print(f\"Output: {pred}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "eV2Zpq4LSDID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55957a6b-34c8-4fac-ef2f-fc4a16ce02f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "  KẾT QUẢ DỊCH THỬ NGHIỆM (BEAM SEARCH WIDTH=3)\n",
            "==================================================\n",
            "Input : Tôi là sinh viên\n",
            "Output: i ' m a student.\n",
            "--------------------------------------------------\n",
            "Input : Hôm nay trời đẹp\n",
            "Output: this is beautiful today.\n",
            "--------------------------------------------------\n",
            "Input : Bạn có thích học lập trình không?\n",
            "Output: do you like to learn to code?\n",
            "--------------------------------------------------\n",
            "Input : Trí tuệ nhân tạo rất thú vị.\n",
            "Output: artificial intelligence is very interesting.\n",
            "--------------------------------------------------\n",
            "Input : Cảm ơn bạn đã giúp đỡ tôi.\n",
            "Output: thank you for helping me.\n",
            "--------------------------------------------------\n",
            "Input : Tôi muốn đi du lịch.\n",
            "Output: i want to travel.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "import tarfile\n",
        "import html\n",
        "\n",
        "checkpoints_to_test = [\n",
        "    {\n",
        "        \"name\": \"Model Epoch 20 (Safe Best)\",\n",
        "        \"path\": \"/content/drive/MyDrive/NLP_Phase6_Final/checkpoints/phase6_safe_best.pt\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Model Epoch 25 (Latest)\",\n",
        "        \"path\": \"/content/drive/MyDrive/NLP_Phase6_Final/checkpoints/phase6_epoch25.pt\"\n",
        "    }\n",
        "]\n",
        "\n",
        "if not os.path.exists('tst2013.vi'):\n",
        "    print(\"Downloading Test Data...\")\n",
        "    os.system(\"wget -q https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz -O test.tgz\")\n",
        "    with tarfile.open(\"test.tgz\", \"r:gz\") as tar: tar.extractall()\n",
        "\n",
        "def clean_file(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return [html.unescape(line).strip() for line in f]\n",
        "\n",
        "test_src = clean_file('tst2013.vi')\n",
        "test_ref = clean_file('tst2013.en')\n",
        "\n",
        "sample_sentences = [\n",
        "    \"Tôi là sinh viên\",\n",
        "    \"Trí tuệ nhân tạo rất thú vị.\",\n",
        "    \"Cảm ơn bạn đã giúp đỡ tôi.\"\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"BẮT ĐẦU SO SÁNH {len(checkpoints_to_test)} MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for ckpt in checkpoints_to_test:\n",
        "    ckpt_name = ckpt[\"name\"]\n",
        "    ckpt_path = ckpt[\"path\"]\n",
        "\n",
        "    print(f\"\\n>>> Đang load: {ckpt_name}\")\n",
        "\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        print(f\" Lỗi: Không tìm thấy file tại {ckpt_path}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        state_dict = torch.load(ckpt_path, map_location=DEVICE)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model.eval() # Chuyển sang chế độ đánh giá\n",
        "    except Exception as e:\n",
        "        print(f\" Lỗi khi load model: {e}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"--- Dịch thử mẫu ({ckpt_name}) ---\")\n",
        "    for sent in sample_sentences:\n",
        "        pred = beam_search_decode(model, sent, src_vocab, trg_vocab, DEVICE, beam_width=3)\n",
        "        print(f\"   Input: {sent}\")\n",
        "        print(f\"   Pred : {pred}\")\n",
        "\n",
        "    print(f\"--- Đang tính BLEU Score trên {len(test_src)} câu... ---\")\n",
        "    hypotheses = []\n",
        "    # Dùng tqdm để hiện thanh tiến trình\n",
        "    for sent in tqdm(test_src, desc=f\"Evaluating {ckpt_name}\"):\n",
        "        pred = beam_search_decode(model, sent, src_vocab, trg_vocab, DEVICE, beam_width=3)\n",
        "        hypotheses.append(pred)\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(hypotheses, [test_ref], tokenize='13a')\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(f\" KẾT QUẢ {ckpt_name.upper()}\")\n",
        "    print(f\" BLEU Score: {bleu.score:.2f}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"\\n=== HOÀN TẤT SO SÁNH ===\")"
      ],
      "metadata": {
        "id": "vC_l15xwSFEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d01400-16f0-45da-f12e-d5a2fd5c7261"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BẮT ĐẦU SO SÁNH 2 MODEL\n",
            "============================================================\n",
            "\n",
            ">>> Đang load: Model Epoch 20 (Safe Best)\n",
            "--- Dịch thử mẫu (Model Epoch 20 (Safe Best)) ---\n",
            "   Input: Tôi là sinh viên\n",
            "   Pred : i ' m a student.\n",
            "   Input: Trí tuệ nhân tạo rất thú vị.\n",
            "   Pred : artificial intelligence is very interesting.\n",
            "   Input: Cảm ơn bạn đã giúp đỡ tôi.\n",
            "   Pred : thank you for helping me.\n",
            "--- Đang tính BLEU Score trên 1268 câu... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Model Epoch 20 (Safe Best): 100%|██████████| 1268/1268 [05:51<00:00,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            " KẾT QUẢ MODEL EPOCH 20 (SAFE BEST)\n",
            " BLEU Score: 15.99\n",
            "----------------------------------------\n",
            "\n",
            ">>> Đang load: Model Epoch 25 (Latest)\n",
            "--- Dịch thử mẫu (Model Epoch 25 (Latest)) ---\n",
            "   Input: Tôi là sinh viên\n",
            "   Pred : i ' m students.\n",
            "   Input: Trí tuệ nhân tạo rất thú vị.\n",
            "   Pred : artificial intelligence is interesting.\n",
            "   Input: Cảm ơn bạn đã giúp đỡ tôi.\n",
            "   Pred : thank you for helping me.\n",
            "--- Đang tính BLEU Score trên 1268 câu... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Model Epoch 25 (Latest): 100%|██████████| 1268/1268 [05:58<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            " KẾT QUẢ MODEL EPOCH 25 (LATEST)\n",
            " BLEU Score: 16.18\n",
            "----------------------------------------\n",
            "\n",
            "=== HOÀN TẤT SO SÁNH ===\n"
          ]
        }
      ]
    }
  ]
}