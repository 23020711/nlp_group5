{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5STbqm9LoBjP",
        "outputId": "41256f48-7df8-430f-90e9-3624da7c73f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking model path: /content/drive/MyDrive/NLP_Assignment_2025(phase2)/checkpoints/transformer_phase2_best.pt\n",
            "--> Đã tìm thấy Model Phase 2!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import os\n",
        "import html\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_PATH = '/content/drive/MyDrive/NLP_Assignment_2025(phase2)'\n",
        "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
        "CHECKPOINT_PATH = os.path.join(PROJECT_PATH, 'checkpoints')\n",
        "MODEL_FILE = os.path.join(CHECKPOINT_PATH, 'transformer_phase2_best.pt')\n",
        "\n",
        "print(f\"Checking model path: {MODEL_FILE}\")\n",
        "if not os.path.exists(MODEL_FILE):\n",
        "    raise FileNotFoundError(\"Chưa tìm thấy file model Phase 2. Hãy kiểm tra lại đường dẫn!\")\n",
        "else:\n",
        "    print(\"--> Đã tìm thấy Model Phase 2!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nzsCcl0G6Xk0"
      },
      "outputs": [],
      "source": [
        "# --- 1. CLEAN TEXT FUNCTION ---\n",
        "def clean_text(text):\n",
        "    text = html.unescape(text)\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    return text.strip()\n",
        "\n",
        "# --- 2. VOCABULARY ---\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=2):\n",
        "        self.itos = {0: \"<unk>\", 1: \"<pad>\", 2: \"<sos>\", 3: \"<eos>\"}\n",
        "        self.stoi = {\"<unk>\": 0, \"<pad>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "    def __len__(self): return len(self.itos)\n",
        "    def numericalize(self, text):\n",
        "        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in text.lower().strip().split()]\n",
        "\n",
        "# --- 3. MODEL ARCHITECTURE ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_head = d_model // n_head\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.w_q(query).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        K = self.w_k(key).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        V = self.w_v(value).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.d_head)\n",
        "        if mask is not None: energy = energy.masked_fill(mask == 0, -1e9)\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        x = torch.matmul(attention, V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.fc_out(x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x): return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src, src_mask):\n",
        "        _src = self.self_attn(src, src, src, src_mask)\n",
        "        src = self.norm1(src + self.dropout(_src))\n",
        "        _src = self.ffn(src)\n",
        "        src = self.norm2(src + self.dropout(_src))\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        _trg = self.self_attn(trg, trg, trg, trg_mask)\n",
        "        trg = self.norm1(trg + self.dropout(_trg))\n",
        "        _trg = self.cross_attn(trg, enc_src, enc_src, src_mask)\n",
        "        trg = self.norm2(trg + self.dropout(_trg))\n",
        "        _trg = self.ffn(trg)\n",
        "        trg = self.norm3(trg + self.dropout(_trg))\n",
        "        return trg\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_layer, n_head, d_ff, dropout, max_len):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layer)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.dropout(self.pos_encoding(self.embedding(src)))\n",
        "        for layer in self.layers: src = layer(src, src_mask)\n",
        "        return src\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, d_model, n_layer, n_head, d_ff, dropout, max_len):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layer)])\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        trg = self.dropout(self.pos_encoding(self.embedding(trg)))\n",
        "        for layer in self.layers: trg = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        return self.fc_out(trg)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model=256, n_head=8, n_layer=3, d_ff=512, dropout=0.1, max_len=100, src_pad_idx=1, trg_pad_idx=1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, n_layer, n_head, d_ff, dropout, max_len)\n",
        "        self.decoder = Decoder(trg_vocab_size, d_model, n_layer, n_head, d_ff, dropout, max_len)\n",
        "    def make_src_mask(self, src): return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=trg.device)).bool()\n",
        "        return trg_pad_mask & trg_sub_mask\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDGKsnj46djA",
        "outputId": "0f87e286-92de-4565-87b9-134e8308937e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Phase 2 Resources...\n",
            "Vocab loaded. Vi size: 12517, En size: 29345\n",
            "Model Phase 2 loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Constants\n",
        "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    sentence = clean_text(sentence)\n",
        "\n",
        "    tokens = [token.lower() for token in sentence.split()]\n",
        "    tokens = [SOS_IDX] + [src_vocab.stoi.get(token, UNK_IDX) for token in tokens] + [EOS_IDX]\n",
        "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "        trg_indices = [SOS_IDX]\n",
        "        for i in range(max_len):\n",
        "            trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n",
        "            trg_mask = model.make_trg_mask(trg_tensor)\n",
        "            output = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "            pred_token = output.argmax(2)[:,-1].item()\n",
        "            trg_indices.append(pred_token)\n",
        "            if pred_token == EOS_IDX: break\n",
        "\n",
        "    trg_tokens = [trg_vocab.itos[i] for i in trg_indices]\n",
        "    result = []\n",
        "    for token in trg_tokens:\n",
        "        if token not in [\"<sos>\", \"<eos>\", \"<pad>\"]:\n",
        "            result.append(token)\n",
        "    return \" \".join(result)\n",
        "\n",
        "print(\"Loading Phase 2 Resources...\")\n",
        "try:\n",
        "    src_vocab = torch.load(os.path.join(DATA_PATH, 'src_vocab.pth'), weights_only=False)\n",
        "    trg_vocab = torch.load(os.path.join(DATA_PATH, 'trg_vocab.pth'), weights_only=False)\n",
        "    print(f\"Vocab loaded. Vi size: {len(src_vocab)}, En size: {len(trg_vocab)}\")\n",
        "\n",
        "    model = Transformer(len(src_vocab), len(trg_vocab), 256, 8, 3, 512, 0.1, 150, PAD_IDX, PAD_IDX)\n",
        "\n",
        "    state_dict = torch.load(MODEL_FILE, map_location=DEVICE, weights_only=False)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.to(DEVICE)\n",
        "    print(\"Model Phase 2 loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi load model: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "almJR1cIX_xE",
        "outputId": "45df666d-3fb0-413e-ef92-a3de79d5efa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and extracting test data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1177269327.py:21: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=TEST_DATA_DIR)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Phase 2 on 1268 sentences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1268/1268 [01:58<00:00, 10.74it/s]\n",
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "PHASE 2 BLEU SCORE: 12.23\n",
            "==============================\n",
            "HTML entities check: PASS (Clean output)\n",
            "\n",
            "--- EXAMPLES ---\n",
            "SRC : Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài \" Chúng ta chẳng có gì phải ghen tị . \"\n",
            "REF : When I was little , I thought my country was the best on the planet , and I grew up singing a song called \" Nothing To Envy . \"\n",
            "PRED: when i was a little bit , i think , the first <unk> country in the world and i usually sing , \" we don 't have to have a lot of chicken . \"\n",
            "--------------------\n",
            "SRC : Tôi đã rất tự hào về đất nước tôi .\n",
            "REF : And I was very proud .\n",
            "PRED: i was very proud of my country .\n",
            "--------------------\n",
            "SRC : Ở trường , chúng tôi dành rất nhiều thời gian để học về cuộc đời của chủ tịch Kim II- Sung , nhưng lại không học nhiều về thế giới bên ngoài , ngoại trừ việc Hoa Kỳ , Hàn Quốc và Nhật Bản là kẻ thù của chúng tôi .\n",
            "REF : In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\n",
            "PRED: in the case , we spent a lot of time to learn about the spinal cord of the <unk> , but not learning about the world , except that the united states , the national government and japan is our own .\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "!pip install -q sacrebleu tqdm\n",
        "import sacrebleu\n",
        "import tarfile\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Cau hinh duong dan\n",
        "TEST_DATA_DIR = os.path.join(PROJECT_PATH, 'data', 'test_2013')\n",
        "tgz_path = os.path.join(TEST_DATA_DIR, \"test-2013-en-vi.tgz\")\n",
        "\n",
        "# 1. Tai va giai nen du lieu\n",
        "if not os.path.exists(TEST_DATA_DIR):\n",
        "    os.makedirs(TEST_DATA_DIR)\n",
        "\n",
        "if not os.path.exists(tgz_path):\n",
        "    print(\"Downloading and extracting test data...\")\n",
        "    url = \"https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz\"\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, tgz_path)\n",
        "        with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
        "            tar.extractall(path=TEST_DATA_DIR)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading data: {e}\")\n",
        "        if os.path.exists(tgz_path): os.remove(tgz_path)\n",
        "\n",
        "# 2. Doc va lam sach du lieu\n",
        "try:\n",
        "    with open(os.path.join(TEST_DATA_DIR, 'tst2013.vi'), 'r', encoding='utf-8') as f:\n",
        "        test_src = [clean_text(line) for line in f.readlines()]\n",
        "\n",
        "    with open(os.path.join(TEST_DATA_DIR, 'tst2013.en'), 'r', encoding='utf-8') as f:\n",
        "        test_trg = [clean_text(line) for line in f.readlines()]\n",
        "\n",
        "    # 3. Chay Inference\n",
        "    model.eval()\n",
        "    hypotheses = []\n",
        "    references = [test_trg]\n",
        "\n",
        "    print(f\"Evaluating Phase 2 on {len(test_src)} sentences...\")\n",
        "    for src_sent in tqdm(test_src):\n",
        "        pred = translate_sentence(src_sent, src_vocab, trg_vocab, model, DEVICE)\n",
        "        hypotheses.append(pred)\n",
        "\n",
        "    # 4. Tinh BLEU\n",
        "    bleu = sacrebleu.corpus_bleu(hypotheses, references, tokenize='13a')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(f\"PHASE 2 BLEU SCORE: {bleu.score:.2f}\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "    # Kiem tra loi HTML\n",
        "    errors = [s for s in hypotheses if \"&apos;\" in s or \"&quot;\" in s]\n",
        "    if len(errors) == 0:\n",
        "        print(\"HTML entities check: PASS (Clean output)\")\n",
        "    else:\n",
        "        print(f\"HTML entities check: FAIL ({len(errors)} errors found)\")\n",
        "\n",
        "    # In vi du so sanh\n",
        "    print(\"\\n--- EXAMPLES ---\")\n",
        "    for i in range(min(3, len(test_src))):\n",
        "        print(f\"SRC : {test_src[i]}\")\n",
        "        print(f\"REF : {test_trg[i]}\")\n",
        "        print(f\"PRED: {hypotheses[i]}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Test data files not found. Please check directory path.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
