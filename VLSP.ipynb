{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8F7ltCrU6OG"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers datasets accelerate peft bitsandbytes sentencepiece sacrebleu rouge_score matplotlib google-generativeai tqdm\n",
        "\n",
        "import torch\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "# Tải OPUS100 EN-VI từ Hugging Face\n",
        "dataset_full = load_dataset(\"opus100\", \"en-vi\")\n",
        "\n",
        "print(\"Đã tải xong OPUS100 EN-VI từ Hugging Face .\")\n",
        "print(f\"Splits: Train={len(dataset_full['train'])}, Val={len(dataset_full['validation'])}, Test={len(dataset_full['test'])}\")\n",
        "\n",
        "# Hàm lọc y tế\n",
        "def filter_medical_domain(dataset_split):\n",
        "    # Keywords y tế tiếng Anh\n",
        "    keywords = [\n",
        "        'doctor', 'nurse', 'hospital', 'patient', 'medicine', 'drug', 'disease', 'virus', 'pain',\n",
        "        'surgery', 'health', 'cancer', 'treatment', 'symptom', 'diagnosis', 'therapy', 'vaccine',\n",
        "        'infection', 'epidemic', 'mental', 'physical', 'wellness', 'care', 'heal', 'cure', 'illness',\n",
        "        'blood', 'heart', 'brain', 'lung', 'stomach', 'bone', 'skin', 'eye', 'ear', 'dental',\n",
        "        'body', 'mind', 'fit', 'diet', 'exercise', 'sleep', 'sick', 'ill', 'medical', 'pharmacy', 'recovery'\n",
        "    ]\n",
        "\n",
        "    filtered_data = []\n",
        "    for example in dataset_split:\n",
        "        en_text = example['translation']['en'].lower().strip()\n",
        "        if any(k in en_text for k in keywords):\n",
        "            filtered_data.append({\n",
        "                \"en\": example['translation']['en'].strip(),\n",
        "                \"vi\": example['translation']['vi'].strip()\n",
        "            })\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "# Lọc từng split\n",
        "medical_train = filter_medical_domain(dataset_full['train'])\n",
        "medical_val = filter_medical_domain(dataset_full['validation'])\n",
        "medical_test = filter_medical_domain(dataset_full['test'])\n",
        "\n",
        "print(f\"  Đã lọc Medical domain:\")\n",
        "print(f\"  Train: {len(medical_train)} câu (từ {len(dataset_full['train'])})\")\n",
        "print(f\"  Val: {len(medical_val)} câu (từ {len(dataset_full['validation'])})\")\n",
        "print(f\"  Test: {len(medical_test)} câu (từ {len(dataset_full['test'])})\")\n",
        "print(f\" Tỷ lệ lọc train: {len(medical_train)/len(dataset_full['train'])*100:.2f}%\")\n",
        "\n",
        "# Hiển thị sample 5 câu đầu train\n",
        "print(\"\\n Sample 5 câu đầu (EN → VI, Medical filtered từ OPUS100):\")\n",
        "for i in range(min(5, len(medical_train))):\n",
        "    print(f\"  [{i+1}] EN: {medical_train[i]['en']}\")\n",
        "    print(f\"     VI: {medical_train[i]['vi']}\\n\")\n",
        "\n",
        "# Tạo DatasetDict mới\n",
        "from datasets import Dataset, DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(medical_train),\n",
        "    \"validation\": Dataset.from_list(medical_val),\n",
        "    \"test\": Dataset.from_list(medical_test)\n",
        "})\n",
        "\n",
        "# Nếu val/test ít (<200 câu sau lọc), bổ sung từ train để cân bằng\n",
        "if len(medical_val) < 200:\n",
        "    extra_val = random.sample(medical_train, min(500, len(medical_train)))[:200]\n",
        "    dataset[\"validation\"] = Dataset.from_list(medical_val + extra_val)\n",
        "if len(medical_test) < 200:\n",
        "    extra_test = random.sample(medical_train, min(500, len(medical_train)))[:200]\n",
        "    dataset[\"test\"] = Dataset.from_list(medical_test + extra_test)\n",
        "\n",
        "print(f\"\\n Splits cuối: Train={len(dataset['train'])}, Val={len(dataset['validation'])}, Test={len(dataset['test'])}\")\n",
        "print(\" Dữ liệu VLSP Medical (OPUS100 filtered) sẵn sàng! \")"
      ],
      "metadata": {
        "id": "fOmyRVpHU85k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINETUNE QWEN2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "\n",
        "print(\"Đang load Qwen/Qwen2-1.5B-Instruct 4-bit…\")\n",
        "\n",
        "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
        "\n",
        "# Tokenizer + pad_token\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model 4-bit\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    ),\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=64, lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "print(\" HOÀN TẤT!\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "fK9EhOc4VAYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess(examples):\n",
        "    inputs = [f\"Translate English to Vietnamese (Medical domain):\\nEnglish: {en}\\nVietnamese:\" for en in examples[\"en\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(examples[\"vi\"], max_length=512, truncation=True).input_ids\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "print(\" Đang preprocess \")\n",
        "tokenized = dataset.map(preprocess, batched=True, remove_columns=[\"en\",\"vi\"], num_proc=4)\n",
        "print(\"Dữ liệu đã sẵn sàng train\")"
      ],
      "metadata": {
        "id": "D8s2IQEkVCA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoding.py\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "import os\n",
        "\n",
        "print(\"Tạo pipeline từ model đã finetune\")\n",
        "\n",
        "translator = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,          # Dùng model đang có trong RAM\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "def beam_translate(text):\n",
        "    prompt = f\"Translate English to Vietnamese (Medical domain):\\nEnglish: {text}\\nVietnamese:\"\n",
        "    out = translator(prompt, max_length=512, num_beams=5, early_stopping=True, do_sample=False)\n",
        "    return out[0][\"generated_text\"].split(\"Vietnamese:\")[-1].strip()\n",
        "\n",
        "# Dịch toàn bộ test set\n",
        "test_en = [ex[\"en\"] for ex in dataset[\"test\"]]\n",
        "print(f\"Đang dịch {len(test_en)} câu test bằng beam search\")\n",
        "\n",
        "preds = []\n",
        "for text in tqdm(test_en, desc=\"Decoding\"):\n",
        "    preds.append(beam_translate(text))\n",
        "\n",
        "# Tính BLEU\n",
        "refs = [[ex[\"vi\"]] for ex in dataset[\"test\"]]\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "\n",
        "print(f\"\\n HOÀN TẤT!\")\n",
        "print(f\"BLEU SCORE = {bleu.score:.2f} \")\n",
        "print(f\"Test size: {len(test_en)} câu\")\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(preds))\n",
        "with open(\"results/test_en.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(test_en))\n",
        "with open(\"results/test_vi.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join([ex[\"vi\"] for ex in dataset[\"test\"]]))\n"
      ],
      "metadata": {
        "id": "dJ_MWr3HVCkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation.py\n",
        "import matplotlib.pyplot as plt\n",
        "import sacrebleu\n",
        "from rouge_score import rouge_scorer\n",
        "import random\n",
        "\n",
        "# Lấy lại dữ liệu để chắc chắn\n",
        "test_en_list = [ex[\"en\"] for ex in dataset[\"test\"]]\n",
        "test_vi = [ex[\"vi\"] for ex in dataset[\"test\"]]\n",
        "\n",
        "print(f\"Final BLEU (corpus): {bleu.score:.2f}\")\n",
        "\n",
        "# ROUGE-L\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "rouge_scores = [scorer.score(r, p)['rougeL'].fmeasure for r, p in zip(test_vi, preds)]\n",
        "print(f\"ROUGE-L: {sum(rouge_scores)/len(rouge_scores):.4f}\")\n",
        "sent_bleus = [sacrebleu.sentence_bleu(p, [r]).score for p, r in zip(preds, test_vi)]\n",
        "\n",
        "# Vẽ đồ thị\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(sent_bleus, color='dodgerblue', alpha=0.8, linewidth=1.5)\n",
        "plt.title(f\"BLEU per Sentence – VLSP 2025 Medical Domain (OPUS100 filtered)\\n\"\n",
        "          f\"Corpus BLEU = {bleu.score:.2f} | Test size = {len(preds)} sentences\",\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Sentence Index\")\n",
        "plt.ylabel(\"Sentence-level BLEU\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/bleu_curve_vlsp2025.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"ĐÃ LƯU ẢNH results/bleu_curve_vlsp2025.png \")\n",
        "\n",
        "# Error analysis 10 câu ngẫu nhiên\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"ERROR ANALYSIS \")\n",
        "print(\"=\"*90)\n",
        "samples_idx = random.sample(range(len(preds)), min(10, len(preds)))\n",
        "\n",
        "for idx in samples_idx:\n",
        "    print(f\"\\n[{idx+1:3d}] EN : {test_en_list[idx]}\")\n",
        "    print(f\"     REF: {test_vi[idx]}\")\n",
        "    print(f\"     PRED: {preds[idx]}\")\n",
        "    print(f\"     BLEU: {sent_bleus[idx]:5.1f} → {'RẤT TỐT' if sent_bleus[idx] > 35 else 'CẦN CẢI THIỆN'}\")\n",
        "    if 'treatment' in test_en_list[idx].lower() and 'điều trị' not in preds[idx].lower():\n",
        "        print(\"     → LỖI: Thiếu từ vựng y khoa (treatment → điều trị)\")\n",
        "    if 'patient' in test_en_list[idx].lower() and 'bệnh nhân' not in preds[idx].lower():\n",
        "        print(\"     → LỖI: Thiếu từ vựng y khoa (patient → bệnh nhân)\")"
      ],
      "metadata": {
        "id": "k1BfqJZ_VFg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"PHÂN TÍCH LỖI DỊCH MÁY – VLSP 2025 MEDICAL DOMAIN\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "def manual_error_analysis(en, ref, pred, bleu_score):\n",
        "    errors = []\n",
        "    if 'treatment' in en.lower() and 'điều trị' not in pred.lower():\n",
        "        errors.append(\"Sai từ vựng y khoa: 'treatment' → nên là 'điều trị'\")\n",
        "    if 'patient' in en.lower() and 'bệnh nhân' not in pred.lower():\n",
        "        errors.append(\"Sai từ vựng y khoa: 'patient' → nên là 'bệnh nhân'\")\n",
        "    if 'doctor' in en.lower() and 'bác sĩ' not in pred.lower():\n",
        "        errors.append(\"Sai từ vựng y khoa: 'doctor' → nên là 'bác sĩ'\")\n",
        "    if 'hospital' in en.lower() and 'bệnh viện' not in pred.lower():\n",
        "        errors.append(\"Sai từ vựng y khoa: 'hospital' → nên là 'bệnh viện'\")\n",
        "    if len(pred.split()) < len(ref.split()) * 0.7:\n",
        "        errors.append(\"Thiếu thông tin (bản dịch ngắn hơn tham chiếu)\")\n",
        "    if len(pred.split()) > len(ref.split()) * 1.5:\n",
        "        errors.append(\"Thừa thông tin (dịch dài dòng)\")\n",
        "    if not errors:\n",
        "        errors.append(\"Không có lỗi đáng kể \")\n",
        "\n",
        "    return errors\n",
        "\n",
        "samples_idx = random.sample(range(len(preds)), 10)\n",
        "\n",
        "for i, idx in enumerate(samples_idx, 1):\n",
        "    en = test_en_list[idx]\n",
        "    ref = test_vi[idx]\n",
        "    pred = preds[idx]\n",
        "    bleu_val = sent_bleus[idx] if 'sent_bleus' in globals() else 0\n",
        "    print(f\"\\n{'='*25} MẪU {i} {'='*25}\")\n",
        "    print(f\"EN : {en}\")\n",
        "    print(f\"REF: {ref}\")\n",
        "    print(f\"PRED: {pred}\")\n",
        "    print(f\"BLEU: {bleu_val:.1f}\")\n",
        "    print(\"→ PHÂN TÍCH LỖI:\")\n",
        "    for err in manual_error_analysis(en, ref, pred, bleu_val):\n",
        "        print(f\"   • {err}\")\n",
        "    print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "UhjNi3DjVGTQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}