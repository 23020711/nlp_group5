{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N8F7ltCrU6OG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f8505a-bcc9-478d-bc45-1c27e5a6f156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: CPU\n"
          ]
        }
      ],
      "source": [
        "# CELL 1\n",
        "!pip install -q torch transformers datasets accelerate peft bitsandbytes sentencepiece sacrebleu rouge_score matplotlib tqdm\n",
        "!pip install -q trl==0.8.6 accelerate --no-deps\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q --upgrade bitsandbytes transformers accelerate peft\n",
        "import torch\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtzWK3fD6A9A",
        "outputId": "d908b349-387d-4807-f1f2-5844432a8f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# CELL 2 ‚Äì LOAD MedEV\n",
        "from datasets import load_dataset, DatasetDict\n",
        "# H√†m load v√† b·ªè header n·∫øu c√≥ (an to√†n, n·∫øu kh√¥ng c√≥ header th√¨ gi·ªØ nguy√™n)\n",
        "def load_text_file(url):\n",
        "    ds = load_dataset(\"text\", data_files=url, split=\"train\")\n",
        "    # Ki·ªÉm tra n·∫øu c√≥ header GIT-LFS (d√≤ng ƒë·∫ßu ch·ª©a \"version https://git-lfs\")\n",
        "    if len(ds) > 0 and \"version https://git-lfs\" in ds[0][\"text\"]:\n",
        "        print(f\"B·ªè header GIT-LFS cho file {url}\")\n",
        "        ds = ds.select(range(3, len(ds)))\n",
        "    return ds\n",
        "\n",
        "train_en = load_text_file(\"https://huggingface.co/datasets/nhuvo/MedEV/resolve/main/train.en.txt\")\n",
        "train_vi = load_text_file(\"https://huggingface.co/datasets/nhuvo/MedEV/resolve/main/train.vi.txt\")\n",
        "val_en = load_text_file(\"https://huggingface.co/datasets/nhuvo/MedEV/resolve/main/val.en.new.txt\")\n",
        "val_vi = load_text_file(\"https://huggingface.co/datasets/nhuvo/MedEV/resolve/main/val.vi.new.txt\")\n",
        "test_en = load_text_file(\"https://huggingface.co/datasets/nhuvo/MedEV/resolve/main/test.en.new.txt\")\n",
        "test_vi = load_text_file(\"https://huggingface.co/datasets/nhuvo/MedEV/resolve/main/test.vi.new.txt\")\n",
        "\n",
        "# Gh√©p parallel\n",
        "train = train_en.add_column(\"vi\", train_vi[\"text\"]).rename_column(\"text\", \"en\")\n",
        "val = val_en.add_column(\"vi\", val_vi[\"text\"]).rename_column(\"text\", \"en\")\n",
        "test = test_en.add_column(\"vi\", test_vi[\"text\"]).rename_column(\"text\", \"en\")\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train,\n",
        "    \"validation\": val,\n",
        "    \"test\": test\n",
        "})\n",
        "\n",
        "# Ki·ªÉm tra\n",
        "print(\"\\nC·∫•u tr√∫c dataset sau gh√©p:\")\n",
        "print(dataset)\n",
        "\n",
        "print(\"\\nM·∫´u 5 example t·ª´ train (medical real data):\")\n",
        "for i in range(5):\n",
        "    ex = dataset[\"train\"][i]\n",
        "    print(f\"M·∫´u {i+1}:\")\n",
        "    print(f\"  EN: {ex['en']}\")\n",
        "    print(f\"  VI: {ex['vi']}\")\n",
        "    print(\"\")\n",
        "\n",
        "print(f\"\\nK√≠ch th∆∞·ªõc MedEV:\")\n",
        "print(f\"Train: {len(dataset['train']):,} pairs\")\n",
        "print(f\"Validation: {len(dataset['validation']):,} pairs\")\n",
        "print(f\"Test: {len(dataset['test']):,} pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK9EhOc4VAYj"
      },
      "outputs": [],
      "source": [
        "# CELL 3 ‚Äì LOAD MODEL + LoRA\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Boou7gAWxO7"
      },
      "outputs": [],
      "source": [
        "# CELL 4 ‚Äì FINE-TUNE\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "train_subset_raw = dataset[\"train\"].shuffle(seed=42).select(range(20000))\n",
        "val_subset_raw = dataset[\"validation\"].shuffle(seed=42).select(range(3000))\n",
        "def formatting_prompts_func(example):\n",
        "    texts = []\n",
        "    for en, vi in zip(example[\"en\"], example[\"vi\"]):\n",
        "        text = f\"Translate English to Vietnamese (Medical domain):\\nEnglish: {en.strip()}\\nVietnamese: {vi.strip()}<|im_end|>\"\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "processed_train = train_subset_raw.map(formatting_prompts_func, batched=True, remove_columns=[\"en\", \"vi\"])\n",
        "processed_val = val_subset_raw.map(formatting_prompts_func, batched=True, remove_columns=[\"en\", \"vi\"])\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen2-medical-fast\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps= 2,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    warmup_steps=50,\n",
        "    logging_steps=10 ,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=400,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=2,\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_pin_memory=False ,\n",
        "    dataloader_num_workers=2,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_train,\n",
        "    eval_dataset=processed_val,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length= 512 ,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "# L∆∞u model\n",
        "save_dir = \"./qwen2-1.5b-medical-vi-fast\"\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zz1z-vLUj6x3"
      },
      "outputs": [],
      "source": [
        "# CELL 5\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "import re\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "save_dir = \"./qwen2-1.5b-medical-vi-fast\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2-1.5B-Instruct\",\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, save_dir)\n",
        "model.eval()\n",
        "device = next(model.parameters()).device\n",
        "# Prompt inference\n",
        "def create_prompt(en_text):\n",
        "    return f\"Translate English to Vietnamese :\\nEnglish: {en_text.strip()}\\nVietnamese:\"\n",
        "NUM_TEST_SAMPLES = 2000\n",
        "print(f\"\\nGenerate tr√™n {NUM_TEST_SAMPLES} c√¢u test...\")\n",
        "test_en = [ex[\"en\"] for ex in dataset[\"test\"].select(range(NUM_TEST_SAMPLES))]\n",
        "test_vi = [ex[\"vi\"] for ex in dataset[\"test\"].select(range(NUM_TEST_SAMPLES))]\n",
        "\n",
        "prompts = [create_prompt(text) for text in test_en]\n",
        "\n",
        "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=448).to(device)\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"do_sample\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id,\n",
        "}\n",
        "\n",
        "batch_size = 8\n",
        "preds = []\n",
        "\n",
        "print(\"Generating...\")\n",
        "for i in tqdm(range(0, len(inputs[\"input_ids\"]), batch_size)):\n",
        "    batch = {k: v[i:i+batch_size] for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(**batch, **generation_kwargs)\n",
        "\n",
        "    texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "    for text in texts:\n",
        "        if \"Vietnamese:\" in text:\n",
        "            pred = text.split(\"Vietnamese:\", 1)[-1].strip()\n",
        "        else:\n",
        "            pred = text.strip()\n",
        "\n",
        "        pred = re.sub(r\"\\s+\", \" \", pred).strip()\n",
        "        # Vi·∫øt hoa ch·ªØ ƒë·∫ßu\n",
        "        if pred and pred[0].islower():\n",
        "            pred = pred.capitalize()\n",
        "\n",
        "        preds.append(pred)\n",
        "\n",
        "# T√≠nh BLEU\n",
        "refs = [[ref] for ref in test_vi]\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"GENERATE {len(preds)} C√ÇU\")\n",
        "print(f\" CORPUS BLEU: {bleu.score:.2f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# In 10 v√≠ d·ª• ƒë·ªÉ ki·ªÉm tra ch·∫•t l∆∞·ª£ng\n",
        "print(\"\\nüîç 10 V√ç D·ª§ ƒê·∫¶U TI√äN:\")\n",
        "for i in range(min(10, len(test_en))):\n",
        "    print(f\"EN   : {test_en[i]}\")\n",
        "    print(f\"REF  : {test_vi[i]}\")\n",
        "    print(f\"PRED : {preds[i]}\")\n",
        "    print(f\"Sentence BLEU: {sacrebleu.sentence_bleu(preds[i], [test_vi[i]]).score:.1f}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1BfqJZ_VFg7"
      },
      "outputs": [],
      "source": [
        "# CELL 6 ‚Äì V·∫º ƒê·ªí TH·ªä BLEU + ROUGE-L + ERROR ANALYSIS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sacrebleu\n",
        "from rouge_score import rouge_scorer\n",
        "import random\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "rouge_l_scores = [scorer.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(test_vi, preds)]\n",
        "avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
        "\n",
        "sent_bleus = [sacrebleu.sentence_bleu(pred, [ref]).score for pred, ref in zip(preds, test_vi)]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(sent_bleus, color='dodgerblue', alpha=0.7, linewidth=1.5, label='Sentence BLEU')\n",
        "plt.axhline(y=bleu.score, color='green', linestyle='--', label=f'Corpus BLEU: {bleu.score:.2f}')\n",
        "plt.axhline(y=sum(sent_bleus)/len(sent_bleus), color='red', linestyle='--', label=f'Avg Sentence BLEU: {sum(sent_bleus)/len(sent_bleus):.1f}')\n",
        "plt.title(f\"Corpus BLEU: {bleu.score:.2f} | ROUGE-L: {avg_rouge_l:.4f} | Samples: {NUM_TEST_SAMPLES}\")\n",
        "plt.xlabel(\"Sentence Index\")\n",
        "plt.ylabel(\"BLEU Score\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"bleu_curve_fast.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(f\"ROUGE-L trung b√¨nh: {avg_rouge_l:.4f}\")\n",
        "\n",
        "# Error analysis ng·∫´u nhi√™n 8 c√¢u\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(\"=\"*90)\n",
        "samples_idx = random.sample(range(len(preds)), 8)\n",
        "for idx in samples_idx:\n",
        "    print(f\"\\n[{idx+1:3d}] EN   : {test_en[idx]}\")\n",
        "    print(f\"      REF  : {test_vi[idx]}\")\n",
        "    print(f\"      PRED : {preds[idx]}\")\n",
        "    print(f\"      BLEU : {sent_bleus[idx]:5.1f}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T89PcE62B9zJ"
      },
      "outputs": [],
      "source": [
        "# CELL 7 ‚Äì SO S√ÅNH V·ªöI BASELINE (ZERO-SHOT QWEN2 + NLLB)\n",
        "\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "print(\"=== BASELINE 1: Zero-shot Qwen2-1.5B-Instruct (c√≥ domain prompt) ===\")\n",
        "zero_prompts = [create_prompt(text) for text in test_en]  # D√πng c√πng prompt nh∆∞ fine-tune\n",
        "\n",
        "zero_inputs = tokenizer(zero_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=448).to(device)\n",
        "\n",
        "zero_preds = []\n",
        "for i in tqdm(range(0, len(zero_inputs[\"input_ids\"]), 8), desc=\"Zero-shot\"):\n",
        "    batch = {k: v[i:i+8] for k, v in zero_inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outs = base_model.generate(**batch, **generation_kwargs)\n",
        "    texts = tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
        "    for text in texts:\n",
        "        pred = text.split(\"Vietnamese:\", 1)[-1].strip() if \"Vietnamese:\" in text else text.strip()\n",
        "        zero_preds.append(re.sub(r\"\\s+\", \" \", pred).strip())\n",
        "\n",
        "zero_bleu = sacrebleu.corpus_bleu(zero_preds, refs)\n",
        "print(f\"Zero-shot BLEU: {zero_bleu.score:.2f}\")\n",
        "\n",
        "print(\"\\n=== BASELINE 2: NLLB-200-distilled-600M ===\")\n",
        "nllb = pipeline(\"translation\", model=\"facebook/nllb-200-distilled-600M\", src_lang=\"eng_Latn\", tgt_lang=\"vie_Latn\", device=device, batch_size=16)\n",
        "nllb_preds = [tr[\"translation_text\"].strip() for tr in tqdm(nllb(test_en[:80]), desc=\"NLLB\")]  # Gi·ªõi h·∫°n 80 ƒë·ªÉ nhanh\n",
        "nllb_bleu = sacrebleu.corpus_bleu(nllb_preds, refs)\n",
        "print(f\"NLLB-200 BLEU: {nllb_bleu.score:.2f}\")\n",
        "\n",
        "print(\"\\n=== T·ªîNG K·∫æT ===\")\n",
        "print(f\"Fine-tuned Qwen2-1.5B (medical) : {bleu.score:.2f}\")\n",
        "print(f\"Zero-shot Qwen2-1.5B           : {zero_bleu.score:.2f}\")\n",
        "print(f\"NLLB-200 dedicated MT          : {nllb_bleu.score:.2f}\")\n",
        "print(\"‚Üí Fine-tune domain mang l·∫°i c·∫£i thi·ªán r√µ r·ªát!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}