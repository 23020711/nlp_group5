{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5STbqm9LoBjP",
        "outputId": "50e47c58-7fa9-4569-cdab-f9d56d48a651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tim thay model tai: /content/drive/MyDrive/NLP_Assignment_2025/checkpoints/transformer_best.pt\n",
            "Dang load Vocab va Model...\n",
            "Load Model thanh cong!\n",
            "\n",
            "--- KET QUA DICH THU ---\n",
            "Input : tôi là sinh viên\n",
            "Output: i was a student .\n",
            "------------------------------\n",
            "Input : hôm nay trời đẹp\n",
            "Output: today &apos;s the sun .\n",
            "------------------------------\n",
            "Input : cảm ơn bạn rất nhiều\n",
            "Output: thank you very much .\n",
            "------------------------------\n",
            "Input : tôi đi học bằng xe buýt\n",
            "Output: i went to the bus .\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_PATH = '/content/drive/MyDrive/NLP_Assignment_2025'\n",
        "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
        "CHECKPOINT_PATH = os.path.join(PROJECT_PATH, 'checkpoints')\n",
        "MODEL_FILE = os.path.join(CHECKPOINT_PATH, 'transformer_best.pt')\n",
        "\n",
        "if not os.path.exists(MODEL_FILE):\n",
        "    print(f\"ERROR: Khong tim thay file model tai {MODEL_FILE}\")\n",
        "else:\n",
        "    print(f\"Tim thay model tai: {MODEL_FILE}\")\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=2):\n",
        "        self.itos = {0: \"<unk>\", 1: \"<pad>\", 2: \"<sos>\", 3: \"<eos>\"}\n",
        "        self.stoi = {\"<unk>\": 0, \"<pad>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self): return len(self.itos)\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in text.lower().strip().split()]\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_head = d_model // n_head\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.w_q(query).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        K = self.w_k(key).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        V = self.w_v(value).view(batch_size, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.d_head)\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        x = torch.matmul(attention, V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.fc_out(x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "# --- Class Transformer Architecture ---\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src, src_mask):\n",
        "        _src = self.self_attn(src, src, src, src_mask)\n",
        "        src = self.norm1(src + self.dropout(_src))\n",
        "        _src = self.ffn(src)\n",
        "        src = self.norm2(src + self.dropout(_src))\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        _trg = self.self_attn(trg, trg, trg, trg_mask)\n",
        "        trg = self.norm1(trg + self.dropout(_trg))\n",
        "        _trg = self.cross_attn(trg, enc_src, enc_src, src_mask)\n",
        "        trg = self.norm2(trg + self.dropout(_trg))\n",
        "        _trg = self.ffn(trg)\n",
        "        trg = self.norm3(trg + self.dropout(_trg))\n",
        "        return trg\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_layer, n_head, d_ff, dropout, max_len):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layer)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.dropout(self.pos_encoding(self.embedding(src)))\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, d_model, n_layer, n_head, d_ff, dropout, max_len):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layer)])\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        trg = self.dropout(self.pos_encoding(self.embedding(trg)))\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        output = self.fc_out(trg)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model=256, n_head=8, n_layer=3, d_ff=512, dropout=0.1, max_len=100, src_pad_idx=1, trg_pad_idx=1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, n_layer, n_head, d_ff, dropout, max_len)\n",
        "        self.decoder = Decoder(trg_vocab_size, d_model, n_layer, n_head, d_ff, dropout, max_len)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=trg.device)).bool()\n",
        "        return trg_pad_mask & trg_sub_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        return output\n",
        "\n",
        "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    tokens = [token.lower() for token in sentence.split()]\n",
        "    tokens = [SOS_IDX] + [src_vocab.stoi.get(token, UNK_IDX) for token in tokens] + [EOS_IDX]\n",
        "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "        trg_indices = [SOS_IDX]\n",
        "        for i in range(max_len):\n",
        "            trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n",
        "            trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "            output = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "\n",
        "            pred_token = output.argmax(2)[:,-1].item()\n",
        "            trg_indices.append(pred_token)\n",
        "\n",
        "            if pred_token == EOS_IDX:\n",
        "                break\n",
        "\n",
        "    trg_tokens = [trg_vocab.itos[i] for i in trg_indices]\n",
        "\n",
        "    # Loai bo SOS va EOS khi in ra\n",
        "    result = []\n",
        "    for token in trg_tokens:\n",
        "        if token not in [\"<sos>\", \"<eos>\", \"<pad>\"]:\n",
        "            result.append(token)\n",
        "\n",
        "    return \" \".join(result)\n",
        "\n",
        "print(\"Dang load Vocab va Model...\")\n",
        "try:\n",
        "    src_vocab = torch.load(os.path.join(DATA_PATH, 'src_vocab.pth'), weights_only=False)\n",
        "    trg_vocab = torch.load(os.path.join(DATA_PATH, 'trg_vocab.pth'), weights_only=False)\n",
        "\n",
        "    INPUT_DIM = len(src_vocab)\n",
        "    OUTPUT_DIM = len(trg_vocab)\n",
        "\n",
        "    D_MODEL = 256\n",
        "    N_HEAD = 8\n",
        "    N_LAYER = 3\n",
        "    D_FF = 512\n",
        "    DROPOUT = 0.1\n",
        "    MAX_LEN = 150\n",
        "\n",
        "    model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, N_HEAD, N_LAYER, D_FF, DROPOUT, MAX_LEN, PAD_IDX, PAD_IDX)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        map_location = torch.device('cuda')\n",
        "    else:\n",
        "        map_location = torch.device('cpu')\n",
        "\n",
        "    state_dict = torch.load(MODEL_FILE, map_location=map_location, weights_only=False)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "    print(\"Load Model thanh cong!\")\n",
        "\n",
        "    sentences = [\n",
        "        \"tôi là sinh viên\",\n",
        "        \"hôm nay trời đẹp\",\n",
        "        \"cảm ơn bạn rất nhiều\",\n",
        "        \"tôi đi học bằng xe buýt\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- KET QUA DICH THU ---\")\n",
        "    for s in sentences:\n",
        "        translated = translate_sentence(s, src_vocab, trg_vocab, model, DEVICE)\n",
        "        print(f\"Input : {s}\")\n",
        "        print(f\"Output: {translated}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Loi xay ra: {e}\")\n",
        "    print(\"Hay kiem tra lai duong dan file trong Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzsCcl0G6Xk0",
        "outputId": "1debc577-22bc-4e8a-ba70-f8f6f7a4999d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đang tải tập test IWSLT 2013...\n",
            "Đang giải nén...\n",
            "Xong! Dữ liệu nằm tại: /content/drive/MyDrive/NLP_Assignment_2025/data/test_2013\n",
            "Các file có trong thư mục:\n",
            "test-2013-en-vi.tgz  tst2013.en  tst2013.vi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3206857410.py:19: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=TEST_DATA_DIR)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "\n",
        "TEST_DATA_DIR = os.path.join(PROJECT_PATH, 'data', 'test_2013')\n",
        "if not os.path.exists(TEST_DATA_DIR):\n",
        "    os.makedirs(TEST_DATA_DIR)\n",
        "\n",
        "url = \"https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz\"\n",
        "tgz_path = os.path.join(TEST_DATA_DIR, \"test-2013-en-vi.tgz\")\n",
        "\n",
        "print(\"Đang tải tập test IWSLT 2013...\")\n",
        "!wget -q {url} -O {tgz_path}\n",
        "\n",
        "print(\"Đang giải nén...\")\n",
        "with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=TEST_DATA_DIR)\n",
        "\n",
        "print(f\"Xong! Dữ liệu nằm tại: {TEST_DATA_DIR}\")\n",
        "print(\"Các file có trong thư mục:\")\n",
        "!ls {TEST_DATA_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDGKsnj46djA",
        "outputId": "931c7643-3000-485d-eff9-1689635c4612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đang đọc dữ liệu test...\n",
            "Số lượng câu test: 1268\n",
            "Bắt đầu dịch (Phase 1 - Baseline)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1268/1268 [02:04<00:00, 10.19it/s]\n",
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "KET QUA PHASE 1 (BASELINE)\n",
            "BLEU SCORE: 11.69\n",
            "========================================\n",
            "\n",
            "--- PHÂN TÍCH LỖI (ERROR ANALYSIS) ---\n",
            "Tìm các câu chứa ký tự lỗi HTML (&apos;, &quot;,...):\n",
            "\n",
            "ID  : 0\n",
            "Input : Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài &quot; Chúng ta chẳng có gì phải ghen tị . &quot;\n",
            "Ref   : When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;\n",
            "Pred  : when i was a little bit old , i think that the best world is the best nation in the world , and i &apos;m always not afraid that we should have to worry about war .\n",
            "-> PHÁT HIỆN LỖI: &apos;\n",
            "------------------------------\n",
            "ID  : 1\n",
            "Input : Tôi đã rất tự hào về đất nước tôi .\n",
            "Ref   : And I was very proud .\n",
            "Pred  : i was so proud of my country .\n",
            "------------------------------\n",
            "ID  : 2\n",
            "Input : Ở trường , chúng tôi dành rất nhiều thời gian để học về cuộc đời của chủ tịch Kim II- Sung , nhưng lại không học nhiều về thế giới bên ngoài , ngoại trừ việc Hoa Kỳ , Hàn Quốc và Nhật Bản là kẻ thù của chúng tôi .\n",
            "Ref   : In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\n",
            "Pred  : in the case , we spent a lot of time to learn about the favelas of the slums of the world , but not so many of the world , except by the united states , the japanese and the japanese , the japanese , the japanese , the japanese\n",
            "------------------------------\n",
            "ID  : 3\n",
            "Input : Mặc dù tôi đã từng tự hỏi không biết thế giới bên ngoài kia như thế nào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộc đời ở BắcTriều Tiên , cho tới khi tất cả mọi thứ đột nhiên thay đổi .\n",
            "Ref   : Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .\n",
            "Pred  : although i &apos;ve been wondering that the world doesn &apos;t know the same world as the same as i thought about life , but i think that i &apos;m going to live in the first life , until it &apos;s all about the things that are done .\n",
            "-> PHÁT HIỆN LỖI: &apos;\n",
            "------------------------------\n",
            "ID  : 4\n",
            "Input : Khi tôi lên 7 , tôi chứng kiến cảnh người ta xử bắn công khai lần đầu tiên trong đời , nhưng tôi vẫn nghĩ cuộc sống của mình ở đây là hoàn toàn bình thường .\n",
            "Ref   : When I was seven years old , I saw my first public execution , but I thought my life in North Korea was normal .\n",
            "Pred  : when i went to seven , i was a founder of my first community , but i think about my life is this is completely normal .\n",
            "------------------------------\n",
            "ID  : 5\n",
            "Input : Gia đình của tôi không nghèo , và bản thân tôi thì chưa từng phải chịu đói .\n",
            "Ref   : My family was not poor , and myself , I had never experienced hunger .\n",
            "Pred  : my family wasn &apos;t poor , and my identity was not going to be saved .\n",
            "-> PHÁT HIỆN LỖI: &apos;\n",
            "------------------------------\n",
            "ID  : 7\n",
            "Input : Trong đó có viết : Khi chị đọc được những dòng này thì cả gia đình 5 người của em đã không còn trên cõi đời này nữa , bởi vì cả nhà em đã không có gì để ăn trong hai tuần .\n",
            "Ref   : It read , &quot; When you read this , all five family members will not exist in this world , because we haven &apos;t eaten for the past two weeks .\n",
            "Pred  : it &apos;s written to write , when she read this line of five million people who haven &apos;t been there , because there was no school in the house , because there was no way that they had no money in the weeks .\n",
            "-> PHÁT HIỆN LỖI: &apos;\n",
            "------------------------------\n",
            "ID  : 9\n",
            "Input : Tôi đã bị sốc .\n",
            "Ref   : I was so shocked .\n",
            "Pred  : i &apos;ve been shocked .\n",
            "-> PHÁT HIỆN LỖI: &apos;\n",
            "------------------------------\n",
            "ID  : 10\n",
            "Input : Vì đó là lần đầu tiên tôi biết rằng đồng bào của tôi đang phải chịu đựng như vậy .\n",
            "Ref   : This was the first time I heard that people in my country were suffering .\n",
            "Pred  : because that &apos;s the first time i know that my cells are suffering like that .\n",
            "-> PHÁT HIỆN LỖI: &apos;\n",
            "------------------------------\n",
            "ID  : 16\n",
            "Input : Việc cúp điện ngày càng xảy ra thường xuyên , vì thế mọi thứ xung quanh tôi đều chìm vào bóng tối khi đêm đến ngoại trừ ánh sáng đèn từ phía Trung Quốc chỉ cách nhà tôi một con sông .\n",
            "Ref   : Power outages also became more and more frequent , so everything around me was completely dark at night except for the sea of lights in China , just across the river from my home .\n",
            "Pred  : the pig did it , so much more than all of the time , so everything i &apos;ve been around my hands when i was sitting with the dark eye of the country , except a national south .\n",
            "-> PHÁT HIỆN LỖI: &apos;\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "!pip install -q sacrebleu tqdm\n",
        "\n",
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "TEST_SRC_FILE = os.path.join(TEST_DATA_DIR, 'tst2013.vi')\n",
        "TEST_TRG_FILE = os.path.join(TEST_DATA_DIR, 'tst2013.en')\n",
        "\n",
        "# --- 1. ĐỌC DỮ LIỆU ---\n",
        "print(\"Đang đọc dữ liệu test...\")\n",
        "with open(TEST_SRC_FILE, 'r', encoding='utf-8') as f:\n",
        "    test_src_sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "with open(TEST_TRG_FILE, 'r', encoding='utf-8') as f:\n",
        "    test_trg_sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Số lượng câu test: {len(test_src_sentences)}\")\n",
        "\n",
        "# --- 2. CHẠY INFERENCE (GREEDY SEARCH) ---\n",
        "model.eval()\n",
        "\n",
        "hypotheses = []\n",
        "references = [test_trg_sentences]\n",
        "\n",
        "print(\"Bắt đầu dịch (Phase 1 - Baseline)...\")\n",
        "for src_text in tqdm(test_src_sentences):\n",
        "    pred = translate_sentence(src_text, src_vocab, trg_vocab, model, DEVICE)\n",
        "    hypotheses.append(pred)\n",
        "\n",
        "# --- 3. TÍNH BLEU SCORE ---\n",
        "bleu = sacrebleu.corpus_bleu(hypotheses, references, tokenize='13a')\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"KET QUA PHASE 1 (BASELINE)\")\n",
        "print(f\"BLEU SCORE: {bleu.score:.2f}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "print(\"\\n--- PHÂN TÍCH LỖI (ERROR ANALYSIS) ---\")\n",
        "print(\"Tìm các câu chứa ký tự lỗi HTML (&apos;, &quot;,...):\\n\")\n",
        "\n",
        "count_errors = 0\n",
        "for i in range(len(hypotheses)):\n",
        "    if \"&apos;\" in hypotheses[i] or \"&quot;\" in hypotheses[i] or i < 5:\n",
        "        print(f\"ID  : {i}\")\n",
        "        print(f\"Input : {test_src_sentences[i]}\")\n",
        "        print(f\"Ref   : {test_trg_sentences[i]}\")\n",
        "        print(f\"Pred  : {hypotheses[i]}\")\n",
        "        if \"&apos;\" in hypotheses[i]:\n",
        "            print(\"-> PHÁT HIỆN LỖI: &apos;\")\n",
        "        print(\"-\" * 30)\n",
        "        count_errors += 1\n",
        "        if count_errors >= 10: break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
